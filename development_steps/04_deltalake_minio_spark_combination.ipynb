{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/manual/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/manual/spark-3.1.1-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/train/.ivy2/cache\n",
      "The jars for the packages stored in: /home/train/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c43c6bd4-9995-4b56-a5b2-28143ec7cb57;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.375 in central\n",
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      ":: resolution report :: resolve 1063ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 from central in [default]\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c43c6bd4-9995-4b56-a5b2-28143ec7cb57\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 10 already retrieved (0kB/26ms)\n",
      "2022-11-10 15:01:09,123 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "import findspark\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "current_ts = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, json_tuple, to_json, from_json, explode_outer\n",
    "\n",
    "# /opt/manual/spark: this is SPARK_HOME path\n",
    "findspark.init(\"/opt/manual/spark\")\n",
    "\n",
    "#spark = SparkSession.builder.config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.2.0\").getOrCreate()\n",
    "# spark & minio configuration on s3\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Spark Minio Test\")\n",
    "         .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://172.18.0.2:9000\")\n",
    "         .config(\"spark.hadoop.fs.s3a.access.key\", 'root')\n",
    "         .config(\"spark.hadoop.fs.s3a.secret.key\", 'root12345')\n",
    "         .config(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "         .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "         .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0,io.delta:delta-core_2.12:1.0.0\")\n",
    "         # .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\")\n",
    "         .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Delta ve s3a configlerinin ayni sparksession altinda calistirilmasi icin, config.jars.packages ',' ile birlestirilmeli!!!\n",
    "# `.config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0,io.delta:delta-core_2.12:1.0.0\")` seklinde uygundur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from delta.tables import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: s3a://tmdb-bronze/credits/credits_part_20221023-230801.parquet",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_10870/2756174713.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m credits = (spark.read.format(\"parquet\")\n\u001B[0m\u001B[1;32m      2\u001B[0m            \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"header\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m            \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"inferSchema\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m            .load(\"s3a://tmdb-bronze/credits/credits_part_20221023-230801.parquet\"))\n\u001B[1;32m      5\u001B[0m \u001B[0mcredits\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venvairspark/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    202\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venvairspark/lib/python3.8/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venvairspark/lib/python3.8/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: Path does not exist: s3a://tmdb-bronze/credits/credits_part_20221023-230801.parquet"
     ]
    }
   ],
   "source": [
    "credits = (spark.read.format(\"parquet\")\n",
    "           .option(\"header\", True)\n",
    "           .option(\"inferSchema\", True)\n",
    "           .load(f\"s3a://tmdb-bronze/credits/credits_part_20221023-230801.parquet\"))\n",
    "credits.show(10, truncate=False)\n",
    "\n",
    "credits.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write delta table to silver bucket\n",
    "credits.write.mode(\"overwrite\").format(\"delta\").save(f\"s3a://tmdb-silver/{current_ts}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "customers = spark.createDataFrame([(10001504, \"Hasan Şahintepesi\", \"Çankırı\", \"Eldivan\", True, \"1992-09-01\", None),\n",
    "                                    (10001505, \"Tuncay Çadırcı\", \"Ankara\", \"Keçiören\", True, \"1992-10-12\", None),\n",
    "                                  (10001506, \"Melahat Bakır\", \"İstanbul\", \"Beykoz\", True, \"1992-08-26\", None),\n",
    "                                   (10001526, \"Sultan Balcı\", \"Muğla\", \"Bodrum\", True, \"1992-09-26\", None),\n",
    "                                  (10001530, \"Dudu Karagölet\", \"Yozgat\", \"Sorgun\", False, \"1992-08-11\", \"1993-08-25\"),\n",
    "                                  (10001518, \"Burcu Vataneri\", \"Kırşehir\", \"Mucur\", False, \"1992-08-22\", \"1993-06-21\")],\n",
    "                                   [\"Id\", \"personName\", \"state\", \"province\", \"still_here\", \"join_date\", \"leave_date\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+--------+--------+----------+----------+----------+\n",
      "|      Id|       personName|   state|province|still_here| join_date|leave_date|\n",
      "+--------+-----------------+--------+--------+----------+----------+----------+\n",
      "|10001504|Hasan Şahintepesi| Çankırı| Eldivan|      true|1992-09-01|      null|\n",
      "|10001505|   Tuncay Çadırcı|  Ankara|Keçiören|      true|1992-10-12|      null|\n",
      "|10001506|    Melahat Bakır|İstanbul|  Beykoz|      true|1992-08-26|      null|\n",
      "|10001526|     Sultan Balcı|   Muğla|  Bodrum|      true|1992-09-26|      null|\n",
      "|10001530|   Dudu Karagölet|  Yozgat|  Sorgun|     false|1992-08-11|1993-08-25|\n",
      "|10001518|   Burcu Vataneri|Kırşehir|   Mucur|     false|1992-08-22|1993-06-21|\n",
      "+--------+-----------------+--------+--------+----------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customers.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#deltaPath = \"hdfs://localhost:9000/user/train/sc2_delta\"\n",
    "#deltaPath = \"file:///home/train/dataops6/delta\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "customers.write \\\n",
    ".mode(\"overwrite\") \\\n",
    ".format(\"delta\") \\\n",
    ".save(deltaPath)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['180645', '180737']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUCKET = 'tmdb-bronze'\n",
    "HTTP = 'http://'\n",
    "ENDPOINT = 'localhost:9000'\n",
    "AWS_ACCESS_KEY_ID = 'root'\n",
    "AWS_SECRET_ACCESS_KEY = 'root12345'\n",
    "REGIONNAME = 'whatever'\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    aws_session_token=None,\n",
    "    region_name=REGIONNAME,\n",
    "    botocore_session=None,\n",
    "    profile_name=None)\n",
    "\n",
    "s3client = session.client(\n",
    "    's3', endpoint_url=f\"{HTTP}{ENDPOINT}\")\n",
    "\n",
    "s3resource = session.resource(\n",
    "    's3', endpoint_url=f\"{HTTP}{ENDPOINT}\")\n",
    "\n",
    "\n",
    "my_bucket = s3resource.Bucket('tmdb-bronze')\n",
    "file_names = []\n",
    "for my_bucket_object in my_bucket.objects.all():\n",
    "    key=my_bucket_object.key\n",
    "    if key.find(f\"credits/credits_part_{current_ts}-\") == 0:\n",
    "        file_names.append(key[-14:-8])\n",
    "file_names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1256331340.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"/tmp/ipykernel_10851/1256331340.py\"\u001B[0;36m, line \u001B[0;32m5\u001B[0m\n\u001B[0;31m    \u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "my_bucket = s3resource.Bucket('tmdb-silver')\n",
    "for my_bucket_object in my_bucket.objects.all():\n",
    "    key=my_bucket_object.key\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "credits.write.mode(\"overwrite\").format(\"delta\").save(f\"s3a://tmdb-silver/{current_ts}/crew\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "106300"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read from delta minio\n",
    "cro = spark.read.format(\"delta\").load(f\"s3a://tmdb-silver/cast/\")\n",
    "cro.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 22:28:31,785 ERROR delta.DeltaLog: Change in the table id detected while updating snapshot. \n",
      "Previous snapshot = Snapshot(path=s3a://tmdb-silver/cast/_delta_log, version=1, metadata=Metadata(b6873935-de3a-498a-b7a9-46755cc2c213,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"movie_id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"title\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"cast_id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"credit_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"gender\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1667761028839)), logSegment=LogSegment(s3a://tmdb-silver/cast/_delta_log,1,WrappedArray(S3AFileStatus{path=s3a://tmdb-silver/cast/_delta_log/00000000000000000000.json; isDirectory=false; length=2859; replication=1; blocksize=33554432; modification_time=1667761034588; access_time=0; owner=train; group=train; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE, S3AFileStatus{path=s3a://tmdb-silver/cast/_delta_log/00000000000000000001.json; isDirectory=false; length=4726; replication=1; blocksize=33554432; modification_time=1667763230474; access_time=0; owner=train; group=train; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE),List(),None,1667763230474), checksumOpt=None)\n",
      "New snapshot = Snapshot(path=s3a://tmdb-silver/cast/_delta_log, version=9, metadata=Metadata(c7454508-c27c-466f-987e-80caa4c94b88,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"movie_id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"title\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"cast_id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"character\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"credit_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"gender\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1667856300498)), logSegment=LogSegment(s3a://tmdb-silver/cast/_delta_log,9,WrappedArray(S3AFileStatus{path=s3a://tmdb-silver/cast/_delta_log/00000000000000000000.json; isDirectory=false; length=1299; replication=1; blocksize=33554432; modification_time=1667856304652; access_time=0; owner=train; group=train; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE, S3AFileStatus{path=s3a://tmdb-silver/cast/_delta_log/00000000000000000001.json; isDirectory=false; length=34845; replication=1; blocksize=33554432; modification_time=1667856318845; access_time=0; owner=train; group=train; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE, S3AFileStatus{path=s3a://tmdb-silver/cast/_delta_log/00000000000000000002.json; isDirectory=false; length=34845; replication=1; blocksize=33554432; modification_time=1667856329869; access_time=0; owner=train; group=train; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE, S3AFileStatus{path=s3a://tmdb-silver/cast/_delta_log/00000000000000000003.json; isDirectory=false; length=34846; replication=1; blocksize=33554432; modification_time=1667856342512; access_time=0; owner=train; group=train; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE, S3AFileStatus{path=s3a://tmdb-silver/cast/_delta_log/00000000000000000004.json; isDirectory=false; length=34846; replication=1; blocksize=33554432; modification_time=1667856355116; access_time=0; owner=train; group=train; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE, S3AFileStatus{path=s3a://tmdb-silver/cast/_delta_log/00000000000000000005.json; isDirectory=false; length=34846; replication=1; blocksize=33554432; modification_time=1667856368504; access_time=0; owner=train; group=train; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE, S3AFileStatus{path=s3a://tmdb-silver/cast/_delta_log/00000000000000000006.json; isDirectory=false; length=34843; replication=1; blocksize=33554432; modification_time=1667856381882; access_time=0; owner=train; group=train; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE, S3AFileStatus{path=s3a://tmdb-silver/cast/_delta_log/00000000000000000007.json; isDirectory=false; length=34843; replication=1; blocksize=33554432; modification_time=1667856395862; access_time=0; owner=train; group=train; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE, S3AFileStatus{path=s3a://tmdb-silver/cast/_delta_log/00000000000000000008.json; isDirectory=false; length=34843; replication=1; blocksize=33554432; modification_time=1667856411667; access_time=0; owner=train; group=train; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE, S3AFileStatus{path=s3a://tmdb-silver/cast/_delta_log/00000000000000000009.json; isDirectory=false; length=34843; replication=1; blocksize=33554432; modification_time=1667856426835; access_time=0; owner=train; group=train; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} isEmptyDirectory=FALSE),List(),None,1667856426835), checksumOpt=None).\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "106300"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cro = spark.read.format(\"delta\").load(f\"s3a://tmdb-silver/cast/\")\n",
    "cro.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "# files in delta logs\n",
    "silver_bucket = s3resource.Bucket('tmdb-silver')\n",
    "delta_log= silver_bucket.objects.filter(Prefix=f\"{current_ts}/crew/part\")\n",
    "delta_log_list = list(delta_log)\n",
    "len(list(delta_log))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "[s3.ObjectSummary(bucket_name='tmdb-silver', key='20221105/crew/part-00000-22afa9cc-71bb-4e07-8112-c1be72bb1281-c000.snappy.parquet'),\n s3.ObjectSummary(bucket_name='tmdb-silver', key='20221105/crew/part-00000-53b3b281-2c0d-46fc-b61a-b5059ed7cd1c-c000.snappy.parquet'),\n s3.ObjectSummary(bucket_name='tmdb-silver', key='20221105/crew/part-00000-c0c55544-7345-483a-8ed2-b05c72e7cffc-c000.snappy.parquet')]"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_log_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# if there is a file in bucket\n",
    "silver_bucket = s3resource.Bucket('tmdb-silver')\n",
    "delta_log= silver_bucket.objects.filter(Prefix=f\"{current_ts}/crew/part\")\n",
    "len(list(delta_log))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-05 23:31:04,679 WARN net.NetUtils: Unable to wrap exception of type class org.apache.hadoop.ipc.RpcException: it has no (String) constructor\n",
      "java.lang.NoSuchMethodException: org.apache.hadoop.ipc.RpcException.<init>(java.lang.String)\n",
      "\tat java.base/java.lang.Class.getConstructor0(Class.java:3349)\n",
      "\tat java.base/java.lang.Class.getConstructor(Class.java:2151)\n",
      "\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:830)\n",
      "\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:806)\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n",
      "\tat com.sun.proxy.$Proxy39.getFileInfo(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:903)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n",
      "\tat com.sun.proxy.$Proxy40.getFileInfo(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1665)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1582)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1579)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1594)\n",
      "\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1700)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:240)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor299.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o649.load.\n: java.io.IOException: Failed on local exception: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length; Host Details : local host is: \"trainvm.vbo.local/127.0.0.1\"; destination host is: \"localhost\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:816)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy39.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:903)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy40.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1665)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1582)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1579)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1594)\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1700)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:240)\n\tat jdk.internal.reflect.GeneratedMethodAccessor299.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1830)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1173)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1069)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_5339/3955947759.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mcro\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"parquet\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"tmdb-silver/20221105/cast/part-00000-96f667ee-8636-4417-813c-324272c22dbe-c000.snappy.parquet\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mcro\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venvspark/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    202\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venvspark/lib/python3.8/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venvspark/lib/python3.8/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    109\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 111\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    112\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/venvspark/lib/python3.8/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o649.load.\n: java.io.IOException: Failed on local exception: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length; Host Details : local host is: \"trainvm.vbo.local/127.0.0.1\"; destination host is: \"localhost\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:816)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy39.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:903)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy40.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1665)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1582)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1579)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1594)\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1700)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:240)\n\tat jdk.internal.reflect.GeneratedMethodAccessor299.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1830)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1173)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1069)\n"
     ]
    }
   ],
   "source": [
    "cro = spark.read.format(\"parquet\").load(\"tmdb-silver/20221105/cast/part-00000-96f667ee-8636-4417-813c-324272c22dbe-c000.snappy.parquet\")\n",
    "cro.show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "'<function a at 0x7f9bc571b1f0>'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def a(y):\n",
    "    print(\"a function is called\")\n",
    "    x = y+5\n",
    "    return x,a.__name__\n",
    "\n",
    "def b(y):\n",
    "    print(\"b function is called\")\n",
    "    x = y+5\n",
    "    return x,b.__name__\n",
    "liste = [a,b]\n",
    "str(a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "'a'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.__name__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yesterday_table = spark.read.format(\"delta\").load(f\"s3a://tmdb-silver/cast/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "   movie_id   title  cast_id            character                 credit_id  \\\n0     19995  Avatar      242           Jake Sully  5602a8a7c3a3685532001c9a   \n1     19995  Avatar        3              Neytiri  52fe48009251416c750ac9cb   \n2     19995  Avatar       25  Dr. Grace Augustine  52fe48009251416c750aca39   \n3     19995  Avatar        4        Col. Quaritch  52fe48009251416c750ac9cf   \n4     19995  Avatar        5         Trudy Chacon  52fe48009251416c750ac9d3   \n\n   gender     id                name  \n0       2  65731     Sam Worthington  \n1       1   8691         Zoe Saldana  \n2       1  10205    Sigourney Weaver  \n3       2  32747        Stephen Lang  \n4       1  17647  Michelle Rodriguez  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movie_id</th>\n      <th>title</th>\n      <th>cast_id</th>\n      <th>character</th>\n      <th>credit_id</th>\n      <th>gender</th>\n      <th>id</th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19995</td>\n      <td>Avatar</td>\n      <td>242</td>\n      <td>Jake Sully</td>\n      <td>5602a8a7c3a3685532001c9a</td>\n      <td>2</td>\n      <td>65731</td>\n      <td>Sam Worthington</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>19995</td>\n      <td>Avatar</td>\n      <td>3</td>\n      <td>Neytiri</td>\n      <td>52fe48009251416c750ac9cb</td>\n      <td>1</td>\n      <td>8691</td>\n      <td>Zoe Saldana</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19995</td>\n      <td>Avatar</td>\n      <td>25</td>\n      <td>Dr. Grace Augustine</td>\n      <td>52fe48009251416c750aca39</td>\n      <td>1</td>\n      <td>10205</td>\n      <td>Sigourney Weaver</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>19995</td>\n      <td>Avatar</td>\n      <td>4</td>\n      <td>Col. Quaritch</td>\n      <td>52fe48009251416c750ac9cf</td>\n      <td>2</td>\n      <td>32747</td>\n      <td>Stephen Lang</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>19995</td>\n      <td>Avatar</td>\n      <td>5</td>\n      <td>Trudy Chacon</td>\n      <td>52fe48009251416c750ac9d3</td>\n      <td>1</td>\n      <td>17647</td>\n      <td>Michelle Rodriguez</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yesterday_table.limit(5).toPandas()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "212600"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yesterday_table.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "71578"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yesterday_table.select(\"character\").distinct().count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Silver bucket controls"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "106300"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cast= spark.read.format(\"delta\").load(f\"s3a://tmdb-silver/cast/\")\n",
    "cast.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# Cast"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 300:================================================>      (28 + 4) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------+---------+---------+------+---+----+\n",
      "|movie_id|title|cast_id|character|credit_id|gender| id|name|\n",
      "+--------+-----+-------+---------+---------+------+---+----+\n",
      "+--------+-----+-------+---------+---------+------+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cast.filter(col(\"credit_id\").isNull()).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "cast = cast.na.fill(value=\"0000000000\", subset=[\"credit_id\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 306:=====================================================> (31 + 1) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------+---------+----------+------+----+----+\n",
      "|movie_id|               title|cast_id|character| credit_id|gender|  id|name|\n",
      "+--------+--------------------+-------+---------+----------+------+----+----+\n",
      "|  447027|     Running Forever|   null|     null|0000000000|  null|null|null|\n",
      "|  346081|           Sardaarji|   null|     null|0000000000|  null|null|null|\n",
      "|  126509|2016: Obama's Ame...|   null|     null|0000000000|  null|null|null|\n",
      "|  371085|           Sharkskin|   null|     null|0000000000|  null|null|null|\n",
      "|  325140|Hum To Mohabbat K...|   null|     null|0000000000|  null|null|null|\n",
      "|  361505|Me You and Five B...|   null|     null|0000000000|  null|null|null|\n",
      "|  114065|Down & Out With T...|   null|     null|0000000000|  null|null|null|\n",
      "|  137955|           Crowsnest|   null|     null|0000000000|  null|null|null|\n",
      "|  102840|  Sex With Strangers|   null|     null|0000000000|  null|null|null|\n",
      "|   43630| The Helix... Loaded|   null|     null|0000000000|  null|null|null|\n",
      "|  292539|         Food Chains|   null|     null|0000000000|  null|null|null|\n",
      "|  279759| Harrison Montgomery|   null|     null|0000000000|  null|null|null|\n",
      "|  296943|The Hadza:  Last ...|   null|     null|0000000000|  null|null|null|\n",
      "|   17644|Barney's Great Ad...|   null|     null|0000000000|  null|null|null|\n",
      "|  331493|Light from the Da...|   null|     null|0000000000|  null|null|null|\n",
      "|   70875|The Harvest (La C...|   null|     null|0000000000|  null|null|null|\n",
      "|   14358|    Mad Hot Ballroom|   null|     null|0000000000|  null|null|null|\n",
      "|  294550|The Outrageous So...|   null|     null|0000000000|  null|null|null|\n",
      "|  380097|America Is Still ...|   null|     null|0000000000|  null|null|null|\n",
      "|  297100|The Little Ponder...|   null|     null|0000000000|  null|null|null|\n",
      "+--------+--------------------+-------+---------+----------+------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cast.filter(col(\"credit_id\")== \"0000000000\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "129609"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crew= spark.read.format(\"delta\").load(f\"s3a://tmdb-silver/crew/\")\n",
    "crew.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 274:=================================================>     (29 + 3) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+---------+----------+------+---+---+----+\n",
      "|movie_id|title|credit_id|department|gender| id|job|name|\n",
      "+--------+-----+---------+----------+------+---+---+----+\n",
      "+--------+-----+---------+----------+------+---+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "crew.filter(col(\"credit_id\").isNull()).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 280:================================================>      (28 + 4) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+----------+------+----+----+----+\n",
      "|movie_id|               title| credit_id|department|gender|  id| job|name|\n",
      "+--------+--------------------+----------+----------+------+----+----+----+\n",
      "|  371085|           Sharkskin|0000000000|      null|  null|null|null|null|\n",
      "|   48382|The Book of Mormo...|0000000000|      null|  null|null|null|null|\n",
      "|  325140|Hum To Mohabbat K...|0000000000|      null|  null|null|null|null|\n",
      "|   20653|      Roadside Romeo|0000000000|      null|  null|null|null|null|\n",
      "|  361505|Me You and Five B...|0000000000|      null|  null|null|null|null|\n",
      "|  114065|Down & Out With T...|0000000000|      null|  null|null|null|null|\n",
      "|  137955|           Crowsnest|0000000000|      null|  null|null|null|null|\n",
      "|  102840|  Sex With Strangers|0000000000|      null|  null|null|null|null|\n",
      "|   43630| The Helix... Loaded|0000000000|      null|  null|null|null|null|\n",
      "|  357441|   Karachi se Lahore|0000000000|      null|  null|null|null|null|\n",
      "|  279759| Harrison Montgomery|0000000000|      null|  null|null|null|null|\n",
      "|   19615|           Flying By|0000000000|      null|  null|null|null|null|\n",
      "|  447027|     Running Forever|0000000000|      null|  null|null|null|null|\n",
      "|   55831|  Boynton Beach Club|0000000000|      null|  null|null|null|null|\n",
      "|  331493|Light from the Da...|0000000000|      null|  null|null|null|null|\n",
      "|  380097|America Is Still ...|0000000000|      null|  null|null|null|null|\n",
      "|  297100|The Little Ponder...|0000000000|      null|  null|null|null|null|\n",
      "|  325579|        Diamond Ruff|0000000000|      null|  null|null|null|null|\n",
      "|  328307|Rise of the Entre...|0000000000|      null|  null|null|null|null|\n",
      "|   47546|   I Want Your Money|0000000000|      null|  null|null|null|null|\n",
      "+--------+--------------------+----------+----------+------+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "crew.filter(col(\"credit_id\")==\"0000000000\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# Genres"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "12188"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres= spark.read.format(\"delta\").load(f\"s3a://tmdb-silver/genres/\")\n",
    "genres.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 175:=================================================>     (29 + 3) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+\n",
      "|movie_id|   id|name|\n",
      "+--------+-----+----+\n",
      "|  191229|-9999|null|\n",
      "|  206412|-9999|null|\n",
      "|  371085|-9999|null|\n",
      "|  346081|-9999|null|\n",
      "|  335874|-9999|null|\n",
      "|  281189|-9999|null|\n",
      "|  137955|-9999|null|\n",
      "|  357834|-9999|null|\n",
      "|  380097|-9999|null|\n",
      "|  282128|-9999|null|\n",
      "|  162396|-9999|null|\n",
      "|  328307|-9999|null|\n",
      "|   48382|-9999|null|\n",
      "|  126186|-9999|null|\n",
      "|  198370|-9999|null|\n",
      "|  325579|-9999|null|\n",
      "|  294550|-9999|null|\n",
      "|  219716|-9999|null|\n",
      "|  279759|-9999|null|\n",
      "|  176074|-9999|null|\n",
      "+--------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "genres.filter(col(\"id\")== -9999).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "28"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres.filter(col(\"id\")== -9999).count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "4803"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies= spark.read.format(\"delta\").load(f\"s3a://tmdb-silver/movies/\")\n",
    "movies.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "28"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres.filter(col(\"id\")== -9999).count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# Spoken Languages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "7023"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spoken_languages= spark.read.format(\"delta\").load(f\"s3a://tmdb-silver/spoken_languages/\")\n",
    "spoken_languages.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "86"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spoken_languages.filter(col(\"iso_639_1\")== \"XX\").count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 235:=====================================================> (28 + 1) / 29]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----+\n",
      "|movie_id|iso_639_1|name|\n",
      "+--------+---------+----+\n",
      "+--------+---------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spoken_languages.filter(col(\"iso_639_1\").isNull()).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "6610"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "production_countries= spark.read.format(\"delta\").load(f\"s3a://tmdb-silver/production_countries/\")\n",
    "production_countries.count()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 261:=====================================================> (28 + 1) / 29]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----+\n",
      "|movie_id|iso_3166_1|name|\n",
      "+--------+----------+----+\n",
      "+--------+----------+----+\n",
      "\n",
      "root\n",
      " |-- movie_id: string (nullable = true)\n",
      " |-- iso_3166_1: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "production_countries.filter(col(\"iso_3166_1\").isNull()).show()\n",
    "production_countries.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 255:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----+\n",
      "|movie_id|iso_3166_1|name|\n",
      "+--------+----------+----+\n",
      "|   24113|        XX|null|\n",
      "|   36597|        XX|null|\n",
      "|  150211|        XX|null|\n",
      "|  281189|        XX|null|\n",
      "|   16433|        XX|null|\n",
      "|  170480|        XX|null|\n",
      "|  371085|        XX|null|\n",
      "|   30979|        XX|null|\n",
      "|  325123|        XX|null|\n",
      "|  219716|        XX|null|\n",
      "|  323270|        XX|null|\n",
      "|  206412|        XX|null|\n",
      "|  320435|        XX|null|\n",
      "|  323271|        XX|null|\n",
      "|   29731|        XX|null|\n",
      "|   43653|        XX|null|\n",
      "|   77332|        XX|null|\n",
      "|   84659|        XX|null|\n",
      "|  279759|        XX|null|\n",
      "|   65448|        XX|null|\n",
      "+--------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "production_countries.filter(col(\"iso_3166_1\")==\"XX\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
